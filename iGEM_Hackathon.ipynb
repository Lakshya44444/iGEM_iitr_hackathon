{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16f4e52d-6895-441d-ae64-4657433ca71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import Keras utilities for image handling and model building\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "\n",
    "# Import ResNet50 for Transfer Learning\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "# CRITICAL: Import ResNet-specific preprocessing for ImageNet normalization\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input \n",
    "\n",
    "# Callbacks for advanced training control\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c451f482-05e3-4be1-ab27-431a213a06b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. CONFIGURATION AND PATHS ---\n",
    "IMAGE_SIZE = (384, 384)  # High resolution input for better feature extraction (Higher AUC potential)\n",
    "BATCH_SIZE = 16          # Reduced batch size accommodates higher image resolution\n",
    "MAX_EPOCHS = 30 \n",
    "MODEL_NAME = 'best_tb_resnet50_auc.h5' # File to save the best model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72149367-d050-44f7-90fd-3e64f98ab0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your correct absolute path here based on file structure\n",
    "BASE_DIR = 'C:/Users/Lakshya Gupta/Downloads/amalgogem/' \n",
    "TRAIN_DIR = os.path.join(BASE_DIR, 'train')\n",
    "VAL_DIR = os.path.join(BASE_DIR, 'val')\n",
    "TEST_DIR = os.path.join(BASE_DIR, 'test')\n",
    "TRAIN_CSV = os.path.join(BASE_DIR, 'train_labels.csv')\n",
    "VAL_CSV = os.path.join(BASE_DIR, 'val_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1350cc10-fc12-48ea-8de7-36a1247f4cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. DATA LOADING & PREPARATION\n",
    "try:\n",
    "    train_df = pd.read_csv(TRAIN_CSV)\n",
    "    val_df = pd.read_csv(VAL_CSV)\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"FATAL ERROR: Could not find CSV files. Path: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Rename columns to match the flow_from_dataframe expected arguments\n",
    "train_df.rename(columns={'image_id': 'filename', 'label': 'class'}, inplace=True)\n",
    "val_df.rename(columns={'image_id': 'filename', 'label': 'class'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b1128b9-dd0d-42f8-96f8-8882a27522c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to ensure filenames have the correct extension (CRITICAL for matching files on disk)\n",
    "def add_extension(df):\n",
    "    if not df['filename'].iloc[0].lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "        df['filename'] = df['filename'].astype(str) + '.png' # Assuming PNG based on previous observation\n",
    "    return df\n",
    "train_df = add_extension(train_df)\n",
    "val_df = add_extension(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "291932d1-e993-439c-b490-3f82de4e0a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated Class Weights: {0: 0.6, 1: 3.0}\n"
     ]
    }
   ],
   "source": [
    "# 3. CLASS WEIGHTS (Mitigating Data Imbalance) ---\n",
    "# FIX: Uses numerical labels (0 and 1) confirmed from CSV content\n",
    "total_samples = len(train_df)\n",
    "tb_count = len(train_df[train_df['class'] == 1]) # Count TB cases (Minority Class)\n",
    "normal_count = len(train_df[train_df['class'] == 0]) # Count Normal cases (Majority Class)\n",
    "\n",
    "if tb_count == 0 or normal_count == 0:\n",
    "    # Fallback weights if data is corrupted\n",
    "    class_weights = {0: 1.0, 1: 1.0}\n",
    "else:\n",
    "    # Weights penalize errors on the minority class (TB) more heavily\n",
    "    weight_for_normal = (1 / normal_count) * (total_samples / 2.0)\n",
    "    weight_for_tb = (1 / tb_count) * (total_samples / 2.0)\n",
    "    class_weights = {0: weight_for_normal, 1: weight_for_tb}\n",
    "\n",
    "print(f\"Calculated Class Weights: {class_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a212b67-be04-48ce-aecf-c4f6bf5b478d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. IMAGE GENERATORS (Forced RGB and ImageNet Preprocessing) ---\n",
    "\n",
    "# FIX: Removed simple rescaling (1./255). Using ResNet's dedicated preprocessing function.\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input, # üîë CRITICAL: Normalizes pixels using ImageNet mean/std\n",
    "    rotation_range=15, width_shift_range=0.1,\n",
    "    height_shift_range=0.1, horizontal_flip=True, fill_mode='nearest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d6bda18-1c1e-4f5b-9634-1e3d9480d820",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d06c088-c707-4110-92fe-abbdac3efedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2688 validated image filenames.\n",
      "Found 672 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=train_df, directory=TRAIN_DIR, x_col='filename', y_col='class',\n",
    "    target_size=IMAGE_SIZE, batch_size=BATCH_SIZE, class_mode='raw', \n",
    "    color_mode='rgb' # FIX: Ensures 3-channel input for ResNet50 weights\n",
    ")\n",
    "val_generator = val_datagen.flow_from_dataframe(\n",
    "    dataframe=val_df, directory=VAL_DIR, x_col='filename', y_col='class',\n",
    "    target_size=IMAGE_SIZE, batch_size=BATCH_SIZE, class_mode='raw', color_mode='rgb'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3925fe76-d36a-4863-96de-978cbc6b0c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Test DF\n",
    "\n",
    "test_files = [f for f in os.listdir(TEST_DIR) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "test_df = pd.DataFrame({'filename': test_files})\n",
    "test_df = add_extension(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ac79e59-9775-4811-b17b-c8cf672ff8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. MODEL CREATION (ResNet50 Transfer Learning) \n",
    "def create_resnet_model(input_shape):\n",
    "    # Load ResNet50 pre-trained on ImageNet weights\n",
    "    base_model = ResNet50(\n",
    "        weights='imagenet', # üîë Using pre-trained weights for feature extraction\n",
    "        include_top=False, # Discard the original classification head\n",
    "        input_shape=input_shape # Explicitly set to (H, W, 3)\n",
    "    )\n",
    "    # Phase 1: Freeze base layers (only the new layers will train quickly)\n",
    "    base_model.trainable = False\n",
    "\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(), # Reduces spatial complexity\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.5), # Regularization to prevent overfitting\n",
    "        Dense(1, activation='sigmoid') # Final output: Probability of TB (0 to 1)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca0df17b-1e5d-41ce-befe-58ccda223c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m94765736/94765736\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "model = create_resnet_model(input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e64b4ce-506e-4940-aedc-04ce8e3778d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. CALLBACKS (Focus on AUC) \n",
    "\n",
    "# Goal: Monitor 'val_auc' (competition metric) and maximize it (mode='max')\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath=MODEL_NAME, monitor='val_auc', mode='max', save_best_only=True, verbose=1)\n",
    "early_stop = EarlyStopping(monitor='val_auc', patience=7, mode='max', restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_auc', patience=4, factor=0.5, verbose=1, mode='max', min_lr=1e-6)\n",
    "callbacks_list = [checkpoint, early_stop, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16974b64-74dd-4911-8e96-74233356fe35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Phase 1: Training Head (Frozen Backbone) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lakshya Gupta\\AppData\\Roaming\\Python\\Python313\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - auc: 0.8976 - loss: 0.5804\n",
      "Epoch 1: val_auc improved from None to 0.99910, saving model to best_tb_resnet50_auc.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m168/168\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m277s\u001b[0m 2s/step - auc: 0.9645 - loss: 0.2637 - val_auc: 0.9991 - val_loss: 0.0329 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - auc: 0.9874 - loss: 0.1417\n",
      "Epoch 2: val_auc improved from 0.99910 to 0.99937, saving model to best_tb_resnet50_auc.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m168/168\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m259s\u001b[0m 2s/step - auc: 0.9887 - loss: 0.1325 - val_auc: 0.9994 - val_loss: 0.0279 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - auc: 0.9980 - loss: 0.0554\n",
      "Epoch 3: val_auc improved from 0.99937 to 0.99971, saving model to best_tb_resnet50_auc.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m168/168\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 2s/step - auc: 0.9970 - loss: 0.0616 - val_auc: 0.9997 - val_loss: 0.0274 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 906ms/step - auc: 0.9960 - loss: 0.0809\n",
      "Epoch 4: val_auc did not improve from 0.99971\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 1s/step - auc: 0.9972 - loss: 0.0618 - val_auc: 0.9996 - val_loss: 0.0202 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 890ms/step - auc: 0.9991 - loss: 0.0323\n",
      "Epoch 5: val_auc did not improve from 0.99971\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 1s/step - auc: 0.9988 - loss: 0.0428 - val_auc: 0.9997 - val_loss: 0.0161 - learning_rate: 0.0010\n",
      "\n",
      "--- Phase 2: Fine-Tuning Entire Model ---\n",
      "Epoch 5/30\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 884ms/step - auc: 0.9985 - loss: 0.0626\n",
      "Epoch 5: val_auc improved from 0.99971 to 0.99972, saving model to best_tb_resnet50_auc.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m168/168\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 1s/step - auc: 0.9984 - loss: 0.0551 - val_auc: 0.9997 - val_loss: 0.0185 - learning_rate: 1.0000e-05\n",
      "Epoch 6/30\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 881ms/step - auc: 0.9991 - loss: 0.0364\n",
      "Epoch 6: val_auc did not improve from 0.99972\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 1s/step - auc: 0.9977 - loss: 0.0427 - val_auc: 0.9997 - val_loss: 0.0192 - learning_rate: 1.0000e-05\n",
      "Epoch 7/30\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 881ms/step - auc: 0.9993 - loss: 0.0364\n",
      "Epoch 7: val_auc did not improve from 0.99972\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 1s/step - auc: 0.9993 - loss: 0.0355 - val_auc: 0.9997 - val_loss: 0.0190 - learning_rate: 1.0000e-05\n",
      "Epoch 8/30\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 885ms/step - auc: 0.9990 - loss: 0.0351\n",
      "Epoch 8: val_auc did not improve from 0.99972\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 1s/step - auc: 0.9989 - loss: 0.0383 - val_auc: 0.9997 - val_loss: 0.0189 - learning_rate: 1.0000e-05\n",
      "Epoch 9/30\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 879ms/step - auc: 0.9993 - loss: 0.0269\n",
      "Epoch 9: val_auc did not improve from 0.99972\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 1s/step - auc: 0.9988 - loss: 0.0347 - val_auc: 0.9997 - val_loss: 0.0190 - learning_rate: 5.0000e-06\n",
      "Epoch 10/30\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 876ms/step - auc: 0.9995 - loss: 0.0301\n",
      "Epoch 10: val_auc did not improve from 0.99972\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 1s/step - auc: 0.9996 - loss: 0.0284 - val_auc: 0.9997 - val_loss: 0.0192 - learning_rate: 5.0000e-06\n",
      "Epoch 11/30\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 886ms/step - auc: 0.9995 - loss: 0.0286\n",
      "Epoch 11: val_auc did not improve from 0.99972\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 1s/step - auc: 0.9992 - loss: 0.0370 - val_auc: 0.9997 - val_loss: 0.0189 - learning_rate: 5.0000e-06\n",
      "Epoch 12/30\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 874ms/step - auc: 0.9934 - loss: 0.0367\n",
      "Epoch 12: val_auc did not improve from 0.99972\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "\u001b[1m168/168\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 1s/step - auc: 0.9994 - loss: 0.0329 - val_auc: 0.9997 - val_loss: 0.0191 - learning_rate: 5.0000e-06\n",
      "\n",
      "Loaded best model weights from best_tb_resnet50_auc.h5\n"
     ]
    }
   ],
   "source": [
    "#  7. MODEL TRAINING (Two Phases for Optimal Performance)\n",
    "\n",
    "# Phase 1: Train the Head (Initial high Learning Rate)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(name='auc')])\n",
    "print(\"\\n--- Phase 1: Training Head (Frozen Backbone) ---\")\n",
    "history = model.fit(train_generator, validation_data=val_generator, epochs=5, class_weight=class_weights, callbacks=callbacks_list, verbose=1)\n",
    "\n",
    "# Phase 2: Fine-Tuning (Unfreeze and Train Entire Model)\n",
    "\n",
    "model.trainable = True # Unlock base layers\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), # üîë Very low LR prevents destroying pre-trained weights\n",
    "              loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(name='auc')])\n",
    "print(\"\\n--- Phase 2: Fine-Tuning Entire Model ---\")\n",
    "model.fit(train_generator, validation_data=val_generator, epochs=MAX_EPOCHS, initial_epoch=history.epoch[-1], class_weight=class_weights, callbacks=callbacks_list, verbose=1)\n",
    "\n",
    "# Load the best model weights based on validation AUC from the saved .h5 file\n",
    "\n",
    "model.load_weights(MODEL_NAME)\n",
    "print(f\"\\nLoaded best model weights from {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93edb254-539c-4a8d-89c1-7acc677a0275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Predictions with TTA (N=8) ---\n",
      "Generating TTA run 1/8...\n",
      "Found 840 validated image filenames.\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 885ms/step\n",
      "Generating TTA run 2/8...\n",
      "Found 840 validated image filenames.\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 885ms/step\n",
      "Generating TTA run 3/8...\n",
      "Found 840 validated image filenames.\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 901ms/step\n",
      "Generating TTA run 4/8...\n",
      "Found 840 validated image filenames.\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 865ms/step\n",
      "Generating TTA run 5/8...\n",
      "Found 840 validated image filenames.\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 878ms/step\n",
      "Generating TTA run 6/8...\n",
      "Found 840 validated image filenames.\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 851ms/step\n",
      "Generating TTA run 7/8...\n",
      "Found 840 validated image filenames.\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 871ms/step\n",
      "Generating TTA run 8/8...\n",
      "Found 840 validated image filenames.\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 868ms/step\n"
     ]
    }
   ],
   "source": [
    "# 8. PREDICTION WITH TEST-TIME AUGMENTATION (TTA) ---\n",
    "\n",
    "def predict_with_tta(model, test_df, test_dir, n_tta):\n",
    "    \"\"\"\n",
    "    TTA is a technique to boost accuracy by averaging predictions \n",
    "    from N augmented versions of each test image.\n",
    "    \"\"\"\n",
    "    tta_preds = []\n",
    "    \n",
    "    # TTA Generator uses simple augmentations\n",
    "    tta_datagen = ImageDataGenerator(\n",
    "        preprocessing_function=preprocess_input, \n",
    "        rotation_range=15, \n",
    "        horizontal_flip=True, \n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "\n",
    "    # Run predictions N times\n",
    "    for i in range(n_tta):\n",
    "        print(f\"Generating TTA run {i+1}/{n_tta}...\")\n",
    "        tta_generator = tta_datagen.flow_from_dataframe(\n",
    "            dataframe=test_df, directory=test_dir, x_col='filename', y_col=None, \n",
    "            target_size=IMAGE_SIZE, batch_size=BATCH_SIZE, class_mode=None, \n",
    "            color_mode='rgb', shuffle=False\n",
    "        )\n",
    "        tta_generator.reset() \n",
    "        preds = model.predict(tta_generator, verbose=1).flatten()\n",
    "        tta_preds.append(preds)\n",
    "    \n",
    "    # Final probability is the mean of all TTA runs\n",
    "    avg_preds = np.mean(tta_preds, axis=0)\n",
    "    return avg_preds\n",
    "\n",
    "N_TTA = 8 # Recommended number of augmentations\n",
    "print(\"\\n--- Starting Predictions with TTA (N=8) ---\")\n",
    "final_probabilities = predict_with_tta(model, test_df, TEST_DIR, N_TTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "392a380d-ae4c-46cb-ac01-9eec0fd30794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Final submission file 'C:/Users/Lakshya Gupta/Downloads/amalgogem/submission_final_ready_3.csv' generated successfully. The image IDs are now numerical (e.g., 1, 2, 3).\n"
     ]
    }
   ],
   "source": [
    "# --- 9. SUBMISSION FILE GENERATION (Final Format Fix) ---\n",
    "submission_df = pd.DataFrame({\n",
    "    # CRITICAL FIX: Strips extension and 'test' prefix, then converts to required integer ID.\n",
    "    'image_id': (\n",
    "        test_df['filename']\n",
    "        .str.replace(r'\\.(png|jpg|jpeg)$', '', regex=True) # Strip extension\n",
    "        .str.replace('test', '', regex=False)              # Strip 'test' prefix (e.g., 'test1' -> '1')\n",
    "        .astype(int)                                       # Convert to integer ID (e.g., 1, 2, 100)\n",
    "    ),\n",
    "    'label': final_probabilities\n",
    "})\n",
    "\n",
    "SUBMISSION_FILE = os.path.join(BASE_DIR, 'submission_final_ready_3.csv') \n",
    "submission_df.to_csv(SUBMISSION_FILE, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Final submission file '{SUBMISSION_FILE}' generated successfully. The image IDs are now numerical (e.g., 1, 2, 3).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba7c34fd-4d44-4c7e-861b-5166006b22db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Final submission file 'C:/Users/Lakshya Gupta/Downloads/amalgogem/submission_binary_match.csv' generated successfully. The labels are now binary (0 or 1).\n"
     ]
    }
   ],
   "source": [
    "# --- 9. SUBMISSION FILE GENERATION (MATCHING VISUAL EXAMPLE) ---\n",
    "\n",
    "# CRITICAL STEP: Convert the predicted probabilities into hard binary labels (0 or 1)\n",
    "# This uses the standard 0.5 threshold.\n",
    "final_binary_labels = np.where(final_probabilities > 0.5, 1, 0)\n",
    "\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    # Use full filename, matching the submission_example.csv image\n",
    "    'image_id': test_df['filename'], \n",
    "    # Use the new binary labels (0 or 1) to match the visual example\n",
    "    'label': final_binary_labels\n",
    "})\n",
    "\n",
    "SUBMISSION_FILE = os.path.join(BASE_DIR, 'submission_binary_match.csv') \n",
    "submission_df.to_csv(SUBMISSION_FILE, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Final submission file '{SUBMISSION_FILE}' generated successfully. The labels are now binary (0 or 1).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d0b795-b188-43b8-ad18-f10a297703f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
